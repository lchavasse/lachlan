---
title: "Getting Started with Langgraph Agents"
date: "2025-01-03"
description: "Getting Started with Langgraph Agents"
---

As with all AI projects, we’re going to need a few API keys.

For this tutorial we’re using [Anthropic](https://console.anthropic.com/) and [Tavily](https://tavily.com/).

Once API keys are obtained they need to be imported into the project.

If you’re working with a regular script, create a .env file in the same directory.

```python
ANTHROPIC_API_KEY=sk-ant-XXX
TAVILY_API_KEY=tvly-XXX
```

Make sure you have **dotenv** installed then add to your imports:

```python
from dotenv import load_dotenv
load_dotenv()
```

Langgraph works with many chat models. We’ll be using Anthropic’s Claude.

As per Langchain, inference is run by the method llm.invoke()

```python
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-5-haiku-latest")

response = llm.invoke("How many r's are there in strawberry?")
print(response)
```

But Langgraph starts getting fun when we introduce tools.

Tavily Search is an indispensable way for models to search the web to retrieve information. It has a ready made langchain [package](https://python.langchain.com/docs/integrations/tools/tavily_search/). This can be run independently with .invoke, but we’re going to be integrating it with our agent. To do this we create a tools array and “bind” it to our llm.

```python
search_tool = TavilySearchResults(max_results=2)

# just for reference
# search_tool.invoke("What is the weather in sf?")

tools = [search_tool]
llm_with_tools = llm.bind_tools(tools)

# but if you run this, you don't actually get a search
response = llm_with_tools.invoke("What is the weather in sf?")
print(response)

===================== RESPONSE ============================="""
content=[{'text': I'll help you check the weather in San Francisco.
I'll use the get_weather function to retrieve the current weather information.",
'type': 'text'}, {'id': 'toolu_0195SC6qgiMxxPAUUNNiro7q', 'input': {'location': 'San Francisco'},
'name': 'get_weather', 'type': 'tool_use'}], additional_kwargs={},
response_metadata={'id': 'msg_01Hp5PcqxELEDQEEYBX4SCz4', 'model': 'claude-3-5-haiku-20241022',
'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0,
'cache_read_input_tokens': 0, 'input_tokens': 428, 'output_tokens': 81}}, id='run-b9c378ec-6ee0-4f7e-8e08-29f5c16e5264-0',
tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_0195SC6qgiMxxPAUUNNiro7q',
'type': 'tool_call'}], usage_metadata={'input_tokens': 428, 'output_tokens': 81, 'total_tokens': 509, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}"""
```

This doesn’t call the search tool just yet, but if we look at response.tool_calls this contains the details to call the correct tool.

```python
print(response.tool_calls)
===============================
[{'name': 'get_weather',
  'args': {'location': 'San Francisco'},
  'id': 'toolu_0153CG6RXcdN6crgG6y3p8mh',
  'type': 'tool_call'}]
```

Now we have to build our graph. This is going to involve 3 key components:

1. A State variable that contains all of the user and system messages:

```python
class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]
```

1. The agent function / node - to call the llm. This is passed the State to which it appends it’s response. 

```python
def agent(state: State):
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    print(response)
    return {"messages": [response]}
```

1. The tool node - to run the selected tool.

For this we can use the prebuilt **ToolNode**, but a custom version is detailed in the Langgraph [Quick Start Guide](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-2-enhancing-the-chatbot-with-tools)

```python
from langgraph.prebuilt import ToolNode
tool_node = ToolNode(tools)
```

1. The Tool Router function - to interface between the agent and tools, deciding if any tools need to be called:
    
    *[The prebuilt [**tools_condition**](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition) function can also be used]*
    

```python
def tool_router(state: State):
    messages = state["messages"]
    last_message = messages[-1]
    if last_message.tool_calls:
        return "tools"
    return END
```

Then we build the **State Machine / Graph**.

This involves 2 key components, nodes and edges. We have two nodes, the agent and the tools.

The edges connect the nodes. We start with then agent, then we use our tool_router function to decide if a tool should be used. If we call a tool, the result should be return to the agent which then decides whether to call another tool or end. This is a ReAct agent. You can read more about agent architecture [here](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/).

```python
builder = StateGraph(State)

builder.add_node("agent", agent)
builder.add_node("tools", tool_node)

builder.add_edge(START, "agent")
builder.add_conditional_edges("agent", tool_router, ["tools", END])
builder.add_edge("tools", "agent")

graph = builder.compile()
```

This can be visualised (*below for within a jupyter notebook*):

```python
from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))
```

![Langgraph State Machine Visualization](/images/langgraph-tool-agent.png)

Then all we have to do is invoke the graph. To do this we stream the response, so that we get them one-at-a-time as soon as they’re generated.

```python
for chunk in app.stream(
    {"messages": [("human", "what is the weather in sf?")]}, stream_mode="values"
):
    chunk["messages"][-1].pretty_print()
```

However, if we want to create a chatbot style interaction, we can loop through this:

```python
def stream_graph_updates(user_input: str):
    for event in graph.stream(
        {"messages": [("user", user_input)]},
    ):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        stream_graph_updates(user_input)
    except:
        print("Error. Goodbye!")
        break
```

## **Memory**

A simple change to the **while** loop passes the previous messages back to the llm on repeated queries giving a semblance of memory:

```python
# Create state once outside the loop
graph_state = {"messages": []}

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        # Update existing state with new message
        graph_state["messages"].append(("user", user_input))
        
        # Use the existing state
        for event in graph.stream(
            graph_state,
            {"configurable": {"thread_id": "1"}},
        ):
            for value in event.values():
                print("Assistant:", value["messages"][-1].content)
                # Update our state with the result
                graph_state = value
    except:
        print("Error. Goodbye!")
        break
```

However memory can be easily added with a [**checkpointer**](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-3-adding-memory-to-the-chatbot) as follows:

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()

# compile graph with checkpointer
graph = builder.compile(checkpointer=memory)

def stream_graph_updates(user_input: str):
    for event in graph.stream(
        {"messages": [("user", user_input)]},
        # the thread must be specified when initialising the state
        {"configurable": {"thread_id": "1"}},
    ):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)
```

Further, this can be connected to a database in production applications.

## Additional Tools

We don’t have to stop at one tool. Our agent can choose the most suitable tool from those available. Custom tools can be built using the [tool class](https://python.langchain.com/docs/concepts/tools/). E.g.

```python
@tool
def get_weather(location: str):
    """Call to get the current weather."""
    if location.lower() in ["sf", "san francisco", "san francisco, ca"]:
        return "It's 60 degrees and foggy."
    else:
        return "It's 90 degrees and sunny."
```

We simple add this to our tools array and the agent will choose which tool to use.

```python
llm_with_tools = llm.bind_tools(tools)
```

The full code for the agent can be found here: [https://github.com/lchavasse/langgraph-tutorial/blob/aa3203c89ac28f8a56935c4a2e6c6ad2f32467ee/basic-ReAct-agent.py](https://github.com/lchavasse/langgraph-tutorial/blob/aa3203c89ac28f8a56935c4a2e6c6ad2f32467ee/basic-ReAct-agent.py)

---

## Postscript

For simple ReAct agents such as the above, Langgraph has a prebuilt [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) which replaces the nodes and entire build process with a single line:

```python
graph = create_react_agent(llm, tools=tools)
```
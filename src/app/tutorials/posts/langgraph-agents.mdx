---
title: "Getting Started with Langgraph Agents"
date: "2025-01-03"
description: "Getting Started with Langgraph Agents"
---

Our first “agent” will be a simple system where an LLM can chose to call one (or more) tools in order to answer the user’s query. (think ChatGPT searching the web). The response is then assessed by the same LLM to decide if the information is relevant and respond to the user.

As outlined [earlier](https://www.lachlan.xyz/tutorials/why-langgraph), we shall be building our system in Python with LangGraph. Familiarity with Python, therefore, is important.

---

As with all AI projects, we’re going to need a few API keys.

For this tutorial we’re using [Anthropic](https://console.anthropic.com/) (for the LLM) and [Tavily](https://tavily.com/) (for Web Search).

Once API keys are obtained they need to be imported into the project.

If you’re working with a regular script, create a **.env** file in the same directory.

```python
ANTHROPIC_API_KEY=sk-ant-XXX
TAVILY_API_KEY=tvly-XXX
```

Make sure you have **dotenv** installed then add to your imports:

```python
from dotenv import load_dotenv
load_dotenv()
```

LangGraph works with many chat models. We’ll be using Anthropic’s Claude.

As per LangChain, inference is run by the method `llm.invoke()`.

```python
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-5-haiku-latest")

response = llm.invoke("How many r's are there in strawberry?")
print(response)
```
---

But Langgraph starts getting fun when we introduce tools.

<br />

Tavily Search is a simple API for searching the web to retrieve information. It has a ready made LangChain [package](https://python.langchain.com/docs/integrations/tools/tavily_search/). This can be run independently with .invoke, but we’re going to be integrating it with our agent. To do this we create a tools array and “bind” it to our llm.

```python
search_tool = TavilySearchResults(max_results=2)

# just for reference
# search_tool.invoke("What is the weather in sf?")

tools = [search_tool]
llm_with_tools = llm.bind_tools(tools)

# but if you run this, you don't actually get a search
response = llm_with_tools.invoke("What is the weather in sf?")
print(response)

===================== RESPONSE ============================="""
content=[{'text': I'll help you check the weather in San Francisco.
I'll use the get_weather function to retrieve the current weather information.",
'type': 'text'}, {'id': 'toolu_0195SC6qgiMxxPAUUNNiro7q', 'input': {'location': 'San Francisco'},
'name': 'get_weather', 'type': 'tool_use'}], additional_kwargs={},
response_metadata={'id': 'msg_01Hp5PcqxELEDQEEYBX4SCz4', 'model': 'claude-3-5-haiku-20241022',
'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0,
'cache_read_input_tokens': 0, 'input_tokens': 428, 'output_tokens': 81}}, id='run-b9c378ec-6ee0-4f7e-8e08-29f5c16e5264-0',
tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_0195SC6qgiMxxPAUUNNiro7q',
'type': 'tool_call'}], usage_metadata={'input_tokens': 428, 'output_tokens': 81, 'total_tokens': 509, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}"""
```

This doesn’t call the search tool just yet, but if we look at response.tool_calls this contains the details to call the correct tool.

```python
print(response.tool_calls)
===============================
[{'name': 'get_weather',
  'args': {'location': 'San Francisco'},
  'id': 'toolu_0153CG6RXcdN6crgG6y3p8mh',
  'type': 'tool_call'}]
```

Now we have to build our graph. This is going to involve 3 key components:

1. A State variable that contains all of the user and system messages:

```python
class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]
```

2. The agent function / node - to call the llm. This is passed the State, to which it appends it’s response. 

```python
def agent(state: State):
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    print(response)
    return {"messages": [response]}
```

3. The tool node - to run the selected tool.

We will use the prebuilt **ToolNode**, but a custom version is detailed in the Langgraph [Quick Start Guide](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-2-enhancing-the-chatbot-with-tools)

```python
from langgraph.prebuilt import ToolNode
tool_node = ToolNode(tools)
```

4. The Tool Router function - to interface between the agent and tools, deciding if any tools need to be called:
    
    *[The prebuilt [**tools_condition**](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition) function can also be used]*
    

```python
def tool_router(state: State):
    messages = state["messages"]
    last_message = messages[-1]
    if last_message.tool_calls:
        return "tools"
    return END
```
---

Now we are ready to build the **State Machine / Graph**.

<br />

This involves 2 key components, *nodes* and *edges*.\
We have two nodes, the agent and the tools. The edges connect the nodes.\
We start with then agent, then we use our *tool§_router* function to decide if a tool should be used. If we call a tool, the result should be return to the agent which then decides whether to call another tool or end. This is a ReAct agent. You can read more about agent architecture [here](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/).

<br />

```python
builder = StateGraph(State)

builder.add_node("agent", agent)
builder.add_node("tools", tool_node)

builder.add_edge(START, "agent")
builder.add_conditional_edges("agent", tool_router, ["tools", END])
builder.add_edge("tools", "agent")

graph = builder.compile()
```

<br />
This can be visualised (*below for within a jupyter notebook*):

```python
from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))
```

![Langgraph State Machine Visualization](/images/langgraph-tool-agent.png)

Then all we have to do is invoke the graph. To do this we stream the response, so that we get the messages one-at-a-time (as soon as they’re generated).

```python
for chunk in app.stream(
    {"messages": [("human", "what is the weather in sf?")]}, stream_mode="values"
):
    chunk["messages"][-1].pretty_print()
```

<br />
However, if we want to create a chatbot style interaction, we can loop through this:

```python
def stream_graph_updates(user_input: str):
    for event in graph.stream(
        {"messages": [("user", user_input)]},
    ):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        stream_graph_updates(user_input)
    except:
        print("Error. Goodbye!")
        break
```
---

## **Memory**

A simple change to the **while** loop passes the previous messages back to the llm on repeated queries giving a semblance of memory:

```python
# Create state once outside the loop
graph_state = {"messages": []}

while True:
    try:
        user_input = input("User: ")
        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        # Update existing state with new message
        graph_state["messages"].append(("user", user_input))
        
        # Use the existing state
        for event in graph.stream(
            graph_state,
            {"configurable": {"thread_id": "1"}},
        ):
            for value in event.values():
                print("Assistant:", value["messages"][-1].content)
                # Update our state with the result
                graph_state = value
    except:
        print("Error. Goodbye!")
        break
```

<br />

However memory can be easily added with a [**checkpointer**](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-3-adding-memory-to-the-chatbot) as follows:

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()

# compile graph with checkpointer
graph = builder.compile(checkpointer=memory)

def stream_graph_updates(user_input: str):
    for event in graph.stream(
        {"messages": [("user", user_input)]},
        # the thread must be specified when initialising the state
        {"configurable": {"thread_id": "1"}},
    ):
        for value in event.values():
            print("Assistant:", value["messages"][-1].content)
```

Further, this can be connected to a database in production applications.

---

## Additional Tools

We don’t have to stop at one tool. Our agent can choose the most suitable tool from those available. Custom tools can be built using the [tool class](https://python.langchain.com/docs/concepts/tools/). E.g.

```python
@tool
def get_weather(location: str):
    """Call to get the current weather."""
    if location.lower() in ["sf", "san francisco", "san francisco, ca"]:
        return "It's 60 degrees and foggy."
    else:
        return "It's 90 degrees and sunny."
```

We simple add this to our tools array and the agent will choose which tool to use.

```python
llm_with_tools = llm.bind_tools(tools)
```

The full code for the agent can be found here: [Basic ReAct Agent](https://github.com/lchavasse/langgraph-tutorial/blob/aa3203c89ac28f8a56935c4a2e6c6ad2f32467ee/basic-ReAct-agent.py)

<br />

## Mastered the ReAct Agent?

Let’s step things up by giving out agent a data store - [RAG Agent](https://www.lachlan.xyz/tutorials/rag-agent-langgraph).

---

## Postscript

For simple ReAct agents such as the above, Langgraph has a prebuilt [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) which replaces the nodes and entire build process with a single line:

```python
graph = create_react_agent(llm, tools=tools)
```